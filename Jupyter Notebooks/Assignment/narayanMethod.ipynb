{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "Loss: 0.4593\n",
      "Accuracy: 0.8589\n",
      "Epoch 2/25\n",
      "Loss: 0.3783\n",
      "Accuracy: 0.8867\n",
      "Epoch 3/25\n",
      "Loss: 0.3530\n",
      "Accuracy: 0.8988\n",
      "Epoch 4/25\n",
      "Loss: 0.3404\n",
      "Accuracy: 0.8999\n",
      "Epoch 5/25\n",
      "Loss: 0.3420\n",
      "Accuracy: 0.8963\n",
      "Epoch 6/25\n",
      "Loss: 0.3405\n",
      "Accuracy: 0.8995\n",
      "Epoch 7/25\n",
      "Loss: 0.3176\n",
      "Accuracy: 0.9066\n",
      "Epoch 8/25\n",
      "Loss: 0.2968\n",
      "Accuracy: 0.9167\n",
      "Epoch 9/25\n",
      "Loss: 0.3015\n",
      "Accuracy: 0.9151\n",
      "Epoch 10/25\n",
      "Loss: 0.2867\n",
      "Accuracy: 0.9207\n",
      "Epoch 11/25\n",
      "Loss: 0.2729\n",
      "Accuracy: 0.9276\n",
      "Epoch 12/25\n",
      "Loss: 0.2964\n",
      "Accuracy: 0.9162\n",
      "Epoch 13/25\n",
      "Loss: 0.2890\n",
      "Accuracy: 0.9190\n",
      "Epoch 14/25\n",
      "Loss: 0.2657\n",
      "Accuracy: 0.9286\n",
      "Epoch 15/25\n",
      "Loss: 0.2711\n",
      "Accuracy: 0.9273\n",
      "Epoch 16/25\n",
      "Loss: 0.2733\n",
      "Accuracy: 0.9263\n",
      "Epoch 17/25\n",
      "Loss: 0.2719\n",
      "Accuracy: 0.9261\n",
      "Epoch 18/25\n",
      "Loss: 0.2766\n",
      "Accuracy: 0.9246\n",
      "Epoch 19/25\n",
      "Loss: 0.2545\n",
      "Accuracy: 0.9334\n",
      "Epoch 20/25\n",
      "Loss: 0.2442\n",
      "Accuracy: 0.9398\n",
      "Epoch 21/25\n",
      "Loss: 0.2508\n",
      "Accuracy: 0.9367\n",
      "Epoch 22/25\n",
      "Loss: 0.2463\n",
      "Accuracy: 0.9388\n",
      "Epoch 23/25\n",
      "Loss: 0.2400\n",
      "Accuracy: 0.9414\n",
      "Epoch 24/25\n",
      "Loss: 0.2366\n",
      "Accuracy: 0.9443\n",
      "Epoch 25/25\n",
      "Loss: 0.2374\n",
      "Accuracy: 0.9416\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).astype('float32') / 255.0\n",
    "\n",
    "\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "y_train = one_hot_encode(y_train)\n",
    "y_test = one_hot_encode(y_test)\n",
    "\n",
    "class BatchNorm:\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        self.eps = eps\n",
    "        self.gamma = np.ones(dim)\n",
    "        self.beta = np.zeros(dim)\n",
    "        self.moving_mean = np.zeros(dim)\n",
    "        self.moving_var = np.ones(dim)\n",
    "        \n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            mean = np.mean(x, axis=0)\n",
    "            var = np.var(x, axis=0) + self.eps\n",
    "            \n",
    "            \n",
    "            momentum = 0.9\n",
    "            self.moving_mean = momentum * self.moving_mean + (1 - momentum) * mean\n",
    "            self.moving_var = momentum * self.moving_var + (1 - momentum) * var\n",
    "            \n",
    "            x_norm = (x - mean) / np.sqrt(var)\n",
    "            self.cache = (x, x_norm, mean, var)\n",
    "        else:\n",
    "            x_norm = (x - self.moving_mean) / np.sqrt(self.moving_var + self.eps)\n",
    "        \n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        x, x_norm, mean, var = self.cache\n",
    "        N = dout.shape[0]\n",
    "        \n",
    "        dgamma = np.sum(dout * x_norm, axis=0)\n",
    "        dbeta = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx_norm = dout * self.gamma\n",
    "        dvar = np.sum(dx_norm * (x - mean) * -0.5 * (var + self.eps)**(-1.5), axis=0)\n",
    "        dmean = np.sum(dx_norm * -1/np.sqrt(var + self.eps), axis=0) + dvar * np.mean(-2 * (x - mean), axis=0)\n",
    "        dx = dx_norm / np.sqrt(var + self.eps) + dvar * 2 * (x - mean) / N + dmean / N\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        return dx\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, reg_type='l2', reg_lambda=0.01):\n",
    "        self.reg_type = reg_type\n",
    "        self.reg_lambda = reg_lambda\n",
    "        \n",
    "        \n",
    "        self.layers = len(hidden_sizes) + 1\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.batch_norms = []\n",
    "        \n",
    "        \n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.weights.append(np.random.randn(prev_size, hidden_size) * np.sqrt(2.0/prev_size))\n",
    "            self.biases.append(np.zeros(hidden_size))\n",
    "            self.batch_norms.append(BatchNorm(hidden_size))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        \n",
    "        self.weights.append(np.random.randn(prev_size, output_size) * np.sqrt(2.0/prev_size))\n",
    "        self.biases.append(np.zeros(output_size))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "        \n",
    "        \n",
    "        for i in range(self.layers - 1):\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            \n",
    "            \n",
    "            z = self.batch_norms[i].forward(z, training)\n",
    "            \n",
    "            a = self.relu(z)\n",
    "            self.activations.append(a)\n",
    "        \n",
    "        \n",
    "        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        self.z_values.append(z)\n",
    "        output = self.softmax(z)\n",
    "        self.activations.append(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, X, y, output, batch_size):\n",
    "        dW = []\n",
    "        db = []\n",
    "        \n",
    "        \n",
    "        delta = output - y\n",
    "        dW.append(np.dot(self.activations[-2].T, delta) / batch_size)\n",
    "        db.append(np.sum(delta, axis=0) / batch_size)\n",
    "        \n",
    "        \n",
    "        for i in range(self.layers - 2, -1, -1):\n",
    "            delta = np.dot(delta, self.weights[i + 1].T)\n",
    "            delta = self.batch_norms[i].backward(delta)\n",
    "            delta = delta * self.relu_derivative(self.z_values[i])\n",
    "            \n",
    "            dW.insert(0, np.dot(self.activations[i].T, delta) / batch_size)\n",
    "            db.insert(0, np.sum(delta, axis=0) / batch_size)\n",
    "        \n",
    "        \n",
    "        if self.reg_type == 'l2':\n",
    "            for i in range(len(dW)):\n",
    "                dW[i] += self.reg_lambda * self.weights[i]\n",
    "        elif self.reg_type == 'l1':\n",
    "            for i in range(len(dW)):\n",
    "                dW[i] += self.reg_lambda * np.sign(self.weights[i])\n",
    "        \n",
    "        return dW, db\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        log_likelihood = -np.log(np.maximum(y_pred[range(m), y_true.argmax(axis=1)], 1e-10))\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        \n",
    "        if self.reg_type == 'l2':\n",
    "            reg_loss = 0\n",
    "            for w in self.weights:\n",
    "                reg_loss += np.sum(w * w)\n",
    "            loss += 0.5 * self.reg_lambda * reg_loss\n",
    "        elif self.reg_type == 'l1':\n",
    "            reg_loss = 0\n",
    "            for w in self.weights:\n",
    "                reg_loss += np.sum(np.abs(w))\n",
    "            loss += self.reg_lambda * reg_loss\n",
    "            \n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, batch_size=128, learning_rate=0.001, epochs=10):\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        \n",
    "        m_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        v_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        m_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        v_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        beta1, beta2 = 0.9, 0.999\n",
    "        epsilon = 1e-8\n",
    "        t = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                t += 1\n",
    "                batch_X = X[i:i + batch_size]\n",
    "                batch_y = y[i:i + batch_size]\n",
    "                \n",
    "                \n",
    "                output = self.forward(batch_X, training=True)\n",
    "                \n",
    "                \n",
    "                dW, db = self.backward(batch_X, batch_y, output, batch_size)\n",
    "                \n",
    "                \n",
    "                for j in range(len(self.weights)):\n",
    "                    \n",
    "                    m_weights[j] = beta1 * m_weights[j] + (1 - beta1) * dW[j]\n",
    "                    v_weights[j] = beta2 * v_weights[j] + (1 - beta2) * (dW[j] * dW[j])\n",
    "                    m_hat = m_weights[j] / (1 - beta1**t)\n",
    "                    v_hat = v_weights[j] / (1 - beta2**t)\n",
    "                    self.weights[j] -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "                    \n",
    "                    \n",
    "                    m_biases[j] = beta1 * m_biases[j] + (1 - beta1) * db[j]\n",
    "                    v_biases[j] = beta2 * v_biases[j] + (1 - beta2) * (db[j] * db[j])\n",
    "                    m_hat = m_biases[j] / (1 - beta1**t)\n",
    "                    v_hat = v_biases[j] / (1 - beta2**t)\n",
    "                    self.biases[j] -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "            \n",
    "            \n",
    "            train_predictions = self.forward(X, training=False)\n",
    "            train_loss = self.compute_loss(y, train_predictions)\n",
    "            train_accuracy = np.mean(np.argmax(train_predictions, axis=1) == np.argmax(y, axis=1))\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            print(f\"Loss: {train_loss:.4f}\")\n",
    "            print(f\"Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "model = NeuralNetwork(\n",
    "    input_size=784,  \n",
    "    hidden_sizes=[512, 256],  \n",
    "    output_size=10,\n",
    "    reg_type='l2',\n",
    "    reg_lambda=0.0001  \n",
    ")\n",
    "\n",
    "\n",
    "model.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,  \n",
    "    learning_rate=0.0005,  \n",
    "    epochs=25  \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
